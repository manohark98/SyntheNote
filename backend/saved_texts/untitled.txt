arXiv:1409.3215v3  [cs.CL]  14 Dec 2014Sequenceto SequenceLearning
withNeuralNetworks
IlyaSutskever
Google
ilyasu@google.comOriolVinyals
Google
vinyals@google.comQuocV.Le
Google
qvl@google.com
Abstract
Deep Neural Networks (DNNs) are powerful models that have ac hieved excel-
lentperformanceondifﬁcultlearningtasks. AlthoughDNNs workwellwhenever
large labeled training sets are available, they cannot be us ed to map sequences to
sequences. In this paper, we present a general end-to-end ap proach to sequence
learningthat makesminimal assumptionson the sequence str ucture. Our method
usesamultilayeredLongShort-TermMemory(LSTM)tomapthe inputsequence
to a vector of a ﬁxed dimensionality,and then another deep LS TM to decode the
target sequence from the vector. Our main result is that on an English to French
translationtaskfromtheWMT’14dataset,thetranslations producedbytheLSTM
achieve a BLEU score of 34.8 on the entire test set, where the L STM’s BLEU
scorewaspenalizedonout-of-vocabularywords. Additiona lly,theLSTMdidnot
have difﬁculty on long sentences. For comparison, a phrase- based SMT system
achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM
to rerank the 1000 hypotheses produced by the aforementione d SMT system, its
BLEU score increases to 36.5, which is close to the previous b est result on this
task. The LSTM also learned sensible phrase and sentence rep resentations that
are sensitive to word order and are relatively invariant to t he active and the pas-
sive voice. Finally, we found that reversing the order of the words in all source
sentences(butnottargetsentences)improvedtheLSTM’spe rformancemarkedly,
because doing so introduced many short term dependencies be tween the source
andthetargetsentencewhichmadetheoptimizationproblem easier.
1 Introduction
Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-
cellentperformanceondifﬁcultproblemssuchasspeechrec ognition[13,7]andvisualobjectrecog-
nition [19, 6, 21, 20]. DNNs are powerful because they can per formarbitrary parallel computation
for a modest number of steps. A surprising example of the powe r of DNNs is their ability to sort
N N-bit numbersusingonly 2 hiddenlayersofquadraticsize [27 ]. So, while neuralnetworksare
related to conventional statistical models, they learn an i ntricate computation. Furthermore, large
DNNscanbetrainedwithsupervisedbackpropagationwhenev erthelabeledtrainingsethasenough
informationto specify the network’sparameters. Thus, if t here exists a parametersetting of a large
DNN that achieves good results (for example, because humans can solve the task very rapidly),
supervisedbackpropagationwillﬁndthese parametersands olvetheproblem.
Despitetheirﬂexibilityandpower,DNNscanonlybeapplied toproblemswhoseinputsandtargets
can be sensibly encoded with vectors of ﬁxed dimensionality . It is a signiﬁcant limitation, since
manyimportantproblemsare best expressed with sequencesw hose lengthsare not knowna-priori.
For example, speech recognitionand machine translation ar e sequential problems. Likewise, ques-
tion answering can also be seen as mapping a sequence of words representing the question to a
1sequence of words representingthe answer. It is therefore c lear that a domain-independentmethod
thatlearnsto mapsequencestosequenceswouldbeuseful.
SequencesposeachallengeforDNNsbecausetheyrequiretha tthedimensionalityoftheinputsand
outputs is known and ﬁxed. In this paper, we show that a straig htforward application of the Long
Short-Term Memory (LSTM) architecture [16] can solve gener al sequence to sequence problems.
TheideaistouseoneLSTMtoreadtheinputsequence,onetime stepatatime,toobtainlargeﬁxed-
dimensional vector representation, and then to use another LSTM to extract the output sequence
fromthatvector(ﬁg.1). ThesecondLSTMisessentiallyarec urrentneuralnetworklanguagemodel
[28, 23, 30] exceptthat it is conditionedon the inputsequen ce. The LSTM’sability to successfully
learn on data with long range temporal dependenciesmakes it a natural choice for this application
duetotheconsiderabletimelagbetweenthe inputsandtheir correspondingoutputs(ﬁg.1).
Therehave beena numberof relatedattemptsto addressthe ge neralsequenceto sequencelearning
problem with neural networks. Our approach is closely relat ed to Kalchbrennerand Blunsom [18]
whoweretheﬁrsttomaptheentireinputsentencetovector,a ndisrelatedtoChoetal.[5]although
the latter was used only for rescoring hypothesesproduced b y a phrase-based system. Graves [10]
introduced a novel differentiable attention mechanism tha t allows neural networks to focus on dif-
ferent parts of their input, and an elegant variant of this id ea was successfully applied to machine
translation by Bahdanau et al. [2]. The Connectionist Seque nce Classiﬁcation is another popular
technique for mapping sequences to sequences with neural ne tworks, but it assumes a monotonic
alignmentbetweentheinputsandtheoutputs[11].
Figure 1: Our model reads an input sentence “ABC” and produces “WXYZ” a s the output sentence. The
model stops making predictions after outputting the end-of -sentence token. Note that the LSTM reads the
input sentence inreverse, because doing so introduces many short term dependencies inthe data that make the
optimizationproblem much easier.
The main result of this work is the following. On the WMT’14 En glish to French translation task,
we obtained a BLEU score of 34.81by directly extractingtranslationsfrom an ensemble of 5 de ep
LSTMs(with384Mparametersand8,000dimensionalstateeac h)usingasimpleleft-to-rightbeam-
search decoder. This is by far the best result achieved by dir ect translation with large neural net-
works. Forcomparison,theBLEUscoreofanSMTbaselineonth isdatasetis33.30[29]. The34.81
BLEUscorewasachievedbyanLSTMwithavocabularyof80kwor ds,sothescorewaspenalized
whenever the reference translation contained a word not cov ered by these 80k. This result shows
that a relatively unoptimized small-vocabulary neural net work architecture which has much room
forimprovementoutperformsa phrase-basedSMTsystem.
Finally,we used the LSTM to rescorethe publiclyavailable 1 000-bestlists of the SMT baseline on
thesametask[29]. Bydoingso,weobtainedaBLEUscoreof36. 5,whichimprovesthebaselineby
3.2BLEUpointsandis closetothepreviousbest publishedre sultonthistask(whichis37.0[9]).
Surprisingly,theLSTMdidnotsufferonverylongsentences ,despitetherecentexperienceofother
researchers with related architectures [26]. We were able t o do well on long sentences because we
reversedtheorderofwordsinthesourcesentencebutnotthe targetsentencesinthetrainingandtest
set. By doingso, we introducedmanyshorttermdependencies that madetheoptimizationproblem
much simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with
long sentences. The simple trick of reversing the words in th e source sentence is one of the key
technicalcontributionsofthiswork.
A useful property of the LSTM is that it learns to map an input s entence of variable length into
a ﬁxed-dimensional vector representation. Given that tran slations tend to be paraphrases of the
source sentences, the translation objective encourages th e LSTM to ﬁnd sentence representations
thatcapturetheirmeaning,assentenceswithsimilarmeani ngsareclosetoeachotherwhiledifferent
2sentencesmeaningswillbefar. Aqualitativeevaluationsu pportsthisclaim,showingthatourmodel
isawareofwordorderandisfairlyinvariantto theactivean dpassivevoice.
2 The model
The Recurrent Neural Network (RNN) [31, 28] is a natural gene ralization of feedforward neural
networks to sequences. Given a sequence of inputs (x1,...,x T), a standard RNN computes a
sequenceofoutputs (y1,...,y T)byiteratingthefollowingequation:
ht= sigm/parenleftbig
Whxxt+Whhht−1/parenrightbig
yt=Wyhht
The RNN can easily map sequences to sequences whenever the al ignment between the inputs the
outputs is known ahead of time. However, it is not clear how to apply an RNN to problemswhose
inputandtheoutputsequenceshavedifferentlengthswithc omplicatedandnon-monotonicrelation-
ships.
The simplest strategy for general sequence learning is to ma p the input sequence to a ﬁxed-sized
vector using one RNN, and then to map the vector to the target s equence with another RNN (this
approachhas also been taken by Cho et al. [5]). While it could work in principle since the RNN is
providedwithalltherelevantinformation,itwouldbedifﬁ culttotraintheRNNsduetotheresulting
longtermdependencies(ﬁgure1)[14, 4, 16,15]. However,th eLongShort-TermMemory(LSTM)
[16] isknownto learnproblemswith longrangetemporaldepe ndencies,so an LSTM maysucceed
inthissetting.
The goal of the LSTM is to estimate the conditional probabili typ(y1,...,y T′|x1,...,x T)where
(x1,...,x T)isaninputsequenceand y1,...,y T′isitscorrespondingoutputsequencewhoselength
T′maydifferfrom T. TheLSTMcomputesthisconditionalprobabilitybyﬁrstobt ainingtheﬁxed-
dimensionalrepresentation voftheinputsequence (x1,...,x T)givenbythelasthiddenstateofthe
LSTM, and then computing the probability of y1,...,y T′with a standard LSTM-LM formulation
whoseinitialhiddenstate isset to therepresentation vofx1,...,x T:
p(y1,...,y T′|x1,...,x T) =T′/productdisplay
t=1p(yt|v,y1,...,y t−1) (1)
In this equation, each p(yt|v,y1,...,y t−1)distribution is represented with a softmax over all the
wordsinthevocabulary. WeusetheLSTMformulationfromGra ves[10]. Notethatwerequirethat
each sentence ends with a special end-of-sentencesymbol “ <EOS>”, which enables the model to
deﬁnea distributionoversequencesofall possiblelengths . Theoverallschemeis outlinedinﬁgure
1, where the shown LSTM computesthe representationof “A”, “ B”, “C”, “ <EOS>” and then uses
thisrepresentationtocomputetheprobabilityof“W”, “X”, “Y”,“Z”,“ <EOS>”.
Our actual models differ from the above description in three important ways. First, we used two
different LSTMs: one for the input sequence and another for t he output sequence, because doing
so increases the numbermodel parametersat negligible comp utationalcost and makes it natural to
traintheLSTMonmultiplelanguagepairssimultaneously[1 8]. Second,wefoundthatdeepLSTMs
signiﬁcantlyoutperformedshallowLSTMs,sowechoseanLST Mwithfourlayers. Third,wefound
itextremelyvaluabletoreversetheorderofthewordsofthe inputsentence. Soforexample,instead
of mapping the sentence a,b,cto the sentence α,β,γ, the LSTM is asked to map c,b,atoα,β,γ,
whereα,β,γisthetranslationof a,b,c. Thisway, aisincloseproximityto α,bisfairlycloseto β,
andsoon,afactthatmakesiteasyforSGDto“establishcommu nication”betweentheinputandthe
output. We foundthissimpledatatransformationtogreatly improvetheperformanceoftheLSTM.
3 Experiments
We applied our method to the WMT’14 English to French MT task i n two ways. We used it to
directly translate the input sentence without using a refer enceSMT system and we it to rescore the
n-bestlistsofanSMTbaseline. Wereporttheaccuracyofthe setranslationmethods,presentsample
translations,andvisualizetheresultingsentencerepres entation.
33.1 Datasetdetails
We used the WMT’14 English to French dataset. We trained our m odels on a subset of 12M sen-
tences consisting of 348M French words and 304M English word s, which is a clean “selected”
subset from [29]. We chose this translation task and this spe ciﬁc training set subset because of the
publicavailabilityofatokenizedtrainingandtestsettog etherwith1000-bestlistsfromthebaseline
SMT[29].
As typical neural language models rely on a vector represent ation for each word, we used a ﬁxed
vocabularyforbothlanguages. Weused160,000ofthemostfr equentwordsforthesourcelanguage
and 80,000 of the most frequent words for the target language . Every out-of-vocabularyword was
replacedwitha special“UNK” token.
3.2 DecodingandRescoring
The core of our experiments involved training a large deep LS TM on many sentence pairs. We
trained it by maximizingthe log probabilityof a correct tra nslationTgiven the source sentence S,
so thetrainingobjectiveis
1/|S|/summationdisplay
(T,S)∈Slogp(T|S)
whereSis the training set. Once training is complete, we producetr anslationsby ﬁndingthe most
likelytranslationaccordingto theLSTM:
ˆT= argmax
Tp(T|S) (2)
We search for the most likely translation using a simple left -to-right beam search decoder which
maintains a small number Bof partial hypotheses, where a partial hypothesis is a preﬁx of some
translation. At each timestep we extend each partial hypoth esis in the beam with every possible
word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but
theBmost likely hypotheses according to the model’s log probabi lity. As soon as the “ <EOS>”
symbolisappendedtoahypothesis,itisremovedfromthebea mandisaddedtothesetofcomplete
hypotheses. While this decoderis approximate,it is simple to implement. Interestingly,oursystem
performswellevenwithabeamsizeof1,andabeamofsize2pro videsmostofthebeneﬁtsofbeam
search(Table1).
We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To
rescoreann-bestlist,wecomputedthelogprobabilityofev eryhypothesiswithourLSTMandtook
anevenaveragewith theirscoreandtheLSTM’sscore.
3.3 ReversingtheSourceSentences
While the LSTM is capable of solving problems with long term d ependencies, we discovered that
the LSTM learns much better when the source sentences are rev ersed (the target sentences are not
reversed). By doing so, the LSTM’s test perplexity dropped f rom 5.8 to 4.7, and the test BLEU
scoresofitsdecodedtranslationsincreasedfrom25.9to30 .6.
While we do not have a complete explanation to this phenomeno n, we believe that it is caused by
the introductionof manyshort term dependenciesto the data set. Normally,when we concatenatea
sourcesentencewithatargetsentence,eachwordinthesour cesentenceisfarfromitscorresponding
word in the target sentence. As a result, the problem has a lar ge “minimal time lag” [17]. By
reversing the words in the source sentence, the average dist ance between corresponding words in
the source and target language is unchanged. However, the ﬁr st few words in the source language
arenowveryclosetotheﬁrstfewwordsinthetargetlanguage ,sotheproblem’sminimaltimelagis
greatly reduced. Thus, backpropagationhas an easier time “ establishing communication”between
the source sentence and the target sentence, which in turn re sults in substantially improved overall
performance.
Initially, we believed that reversing the input sentences w ould only lead to more conﬁdent predic-
tionsintheearlypartsofthetargetsentenceandtolesscon ﬁdentpredictionsinthelaterparts. How-
ever, LSTMs trained on reversed source sentences did much be tter on long sentences than LSTMs
4trainedontherawsourcesentences(seesec. 3.7),whichsug geststhatreversingthe inputsentences
resultsinLSTMswith bettermemoryutilization.
3.4 Trainingdetails
We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,
with 1000 cells at each layer and 1000 dimensional word embed dings, with an input vocabulary
of 160,000 and an output vocabulary of 80,000. Thus the deep L STM uses 8000 real numbers to
represent a sentence. We found deep LSTMs to signiﬁcantly ou tperform shallow LSTMs, where
each additional layer reduced perplexity by nearly 10%, pos sibly due to their much larger hidden
state. We used a naive softmax over 80,000 words at each outpu t. The resulting LSTM has 384M
parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M
forthe“decoder”LSTM).Thecompletetrainingdetailsareg ivenbelow:
•We initialized all of the LSTM’s parameters with the uniform distribution between -0.08
and0.08
•We used stochastic gradient descent without momentum,with a ﬁxed learning rate of 0.7.
After5epochs,webegunhalvingthelearningrateeveryhalf epoch. Wetrainedourmodels
fora totalof7.5epochs.
•We used batches of 128 sequences for the gradient and divided it the size of the batch
(namely,128).
•Although LSTMs tend to not suffer from the vanishing gradien t problem, they can have
exploding gradients. Thus we enforced a hard constraint on t he norm of the gradient [10,
25]byscalingitwhenitsnormexceededathreshold. Foreach trainingbatch,wecompute
s=/bardblg/bardbl2, wheregisthegradientdividedby128. If s >5,we setg=5g
s.
•Different sentences have different lengths. Most sentence s are short (e.g., length 20-30)
but some sentences are long (e.g., length >100), so a minibatch of 128 randomlychosen
training sentenceswill have many short sentences and few lo ng sentences, and as a result,
muchofthecomputationintheminibatchiswasted. Toaddres sthisproblem,wemadesure
thatall sentencesina minibatchareroughlyofthesameleng th,yieldinga 2xspeedup.
3.5 Parallelization
A C++ implementation of deep LSTM with the conﬁguration from the previous section on a sin-
gle GPU processes a speed of approximately 1,700 words per se cond. This was too slow for our
purposes, so we parallelized our model using an 8-GPU machin e. Each layer of the LSTM was
executed on a different GPU and communicated its activation s to the next GPU / layer as soon as
they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate
GPU. The remaining 4 GPUs were used to parallelize the softma x, so each GPU was responsible
formultiplyingbya 1000×20000matrix. Theresultingimplementationachievedaspeedof6, 300
(bothEnglishandFrench)wordspersecondwithaminibatchs izeof128. Trainingtookaboutaten
dayswiththisimplementation.
3.6 ExperimentalResults
We used the cased BLEU score [24] to evaluate the quality of ou r translations. We computed our
BLEU scores using multi-bleu.pl1on thetokenized predictions and ground truth. This way
of evaluating the BELU score is consistent with [5] and [2], a nd reproducesthe 33.3 score of [29].
However, if we evaluate the best WMT’14 system [9] (whose pre dictionscan be downloadedfrom
statmt.org\matrix ) in this manner, we get 37.0, which is greater than the 35.8 re ported by
statmt.org\matrix .
Theresultsarepresentedintables1and2. Ourbestresultsa reobtainedwithanensembleofLSTMs
thatdifferintheirrandominitializationsandintherando morderofminibatches. Whilethedecoded
translations of the LSTM ensemble do not outperformthe best WMT’14 system, it is the ﬁrst time
thata pureneuraltranslationsystemoutperformsa phrase- basedSMTbaselineona largescaleMT
1There several variants of the BLEUscore, andeach variant is deﬁnedwitha perl script.
5Method test BLEUscore (ntst14)
Bahdanau et al. [2] 28.45
Baseline System [29] 33.30
Singleforward LSTM,beam size 12 26.17
Single reversed LSTM,beam size 12 30.59
Ensemble of 5reversedLSTMs,beam size1 33.00
Ensemble of 2reversed LSTMs,beam size 12 33.27
Ensemble of 5reversedLSTMs,beam size2 34.50
Ensemble of 5reversed LSTMs,beam size 12 34.81
Table 1: The performanceof the LSTM on WMT’14 English to Fren ch test set (ntst14). Note that
an ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a s ingle LSTM with a beam of
size 12.
Method testBLEU score (ntst14)
Baseline System [29] 33.30
Choet al. [5] 34.54
BestWMT’14result [9] 37.0
Rescoringthe baseline 1000-best witha single forwardLSTM 35.61
Rescoring the baseline 1000-best withasingle reversedLST M 35.85
Rescoring the baseline 1000-best withanensemble of 5rever sed LSTMs 36.5
Oracle Rescoringof the Baseline 1000-best lists ∼45
Table 2: Methods that use neural networks together with an SM T system on the WMT’14 English
toFrenchtest set (ntst14).
task by a sizeable margin, despite its inability to handle ou t-of-vocabulary words. The LSTM is
within 0.5 BLEU points of the best WMT’14 result if it is used t o rescore the 1000-best list of the
baselinesystem.
3.7 Performanceonlongsentences
We were surprised to discoverthat the LSTM did well onlong se ntences, whichis shown quantita-
tivelyinﬁgure3. Table 3presentsseveralexamplesoflongs entencesandtheirtranslations.
3.8 Model Analysis
−8−6−4−2 0246810−6−5−4−3−2−101234
John respects MaryMary respects John
John admires MaryMary admires John
Mary is in love with John
John is in love with Mary
−15 −10 −5 0 5 10 15 20−20−15−10−5051015
I gave her a card in the gardenIn the garden , I gave her a cardShe was given a card by me in the gardenShe gave me a card in the gardenIn the garden , she gave me a cardI was given a card by her in the garden
Figure 2: The ﬁgure shows a 2-dimensional PCA projection of the LSTM hi dden states that are obtained
after processing the phrases in the ﬁgures. The phrases are c lustered by meaning, which in these examples is
primarilya functionofwordorder, whichwouldbe difﬁcultt ocapture withabag-of-words model. Notice that
both clustershave similarinternal structure.
One of the attractive features of our model is its ability to t urn a sequence of words into a vector
of ﬁxed dimensionality. Figure 2 visualizes some of the lear ned representations. The ﬁgure clearly
showsthattherepresentationsaresensitivetotheorderof words,whilebeingfairlyinsensitivetothe
6Type Sentence
Ourmodel UlrichUNK, membre duconseil d’ administrationdu construc teur automobile Audi ,
afﬁrme qu’ ils’ agit d’une pratique courante depuis des ann´ ees pour que lest´ el´ ephones
portables puissent ˆ etre collect´ es avant les r´ eunions du conseil d’ administrationaﬁnqu’ ils
ne soient pas utilis´ escomme appareils d’ ´ ecoute ` a distan ce .
Truth UlrichHackenberg , membre duconseil d’ administrationdu c onstructeur automobile Audi ,
d´ eclare que la collecte des t´ el´ ephones portables avant l es r´ eunions duconseil , aﬁnqu’ ils
ne puissent pas ˆ etre utilis´ escomme appareils d’ ´ ecoute ` a distance , estune pratique courante
depuis des ann´ ees .
Ourmodel “ Lest´ el´ ephones cellulaires ,qui sont vraiment une quest ion ,non seulement parce qu’ ils
pourraient potentiellement causer des interf´ erences ave c lesappareils de navigation, mais
nous savons ,selon laFCC, qu’ ilspourraient interf´ erer av ec lestours de t´ el´ ephone cellulaire
lorsqu’ ils sont dans l’air” , ditUNK .
Truth “ Lest´ el´ ephones portables sont v´ eritablement un probl` eme , non seulement parce qu’ ils
pourraient ´ eventuellement cr´ eer des interf´ erences ave c lesinstruments de navigation, mais
parce que nous savons ,d’ apr` es laFCC,qu’ ilspourraient pe rturber les antennes-relais de
t´ el´ ephonie mobile s’ ilssont utilis´ es ` a bord” , ad´ ecla r´ e Rosenker .
Ourmodel Avec lacr´ emation ,ilya un “ sentiment de violence contre le corps d’ un ˆ etrecher ” ,
qui sera “ r´ eduit ` a une pile de cendres ” entr` es peu de temps aulieud’ unprocessus de
d´ ecomposition “ qui accompagnera les ´ etapes dudeuil ”.
Truth Ilya , avec lacr´ emation ,“ une violence faite aucorps aim´ e ” ,
qui va ˆ etre“ r´ eduit ` a untas de cendres ” entr` es peude temp s ,et non apr` es unprocessus de
d´ ecomposition ,qui “ accompagnerait les phases du deuil ” .
Table 3: A few examples of long translations produced by the L STM alongside the ground truth
translations. Thereadercanverifythatthe translationsa resensibleusingGoogletranslate.
478 12 17 22 28 35 79
test sentences sorted by their length2025303540BLEU scoreLSTM  (34.8)
baseline (33.3)
0 500 1000 1500 2000 2500 3000 3500
test sentences sorted by average word frequency rank2025303540BLEU scoreLSTM  (34.8)
baseline (33.3)
Figure 3: The left plot shows the performance of our system as a functio n of sentence length, where the
x-axis corresponds to the test sentences sorted by their len gth and is marked by the actual sequence lengths.
There is no degradation onsentences withless than 35words, there is onlya minor degradation on the longest
sentences. The right plot shows the LSTM’s performance on se ntences with progressively more rare words,
where the x-axis corresponds tothe testsentences sorted by their “average word frequency rank”.
replacement of an active voice with a passive voice. The two- dimensionalprojectionsare obtained
usingPCA.
4 Related work
There is a large body of work on applications of neural networ ks to machine translation. So far,
the simplest and most effective way of applying an RNN-Langu age Model (RNNLM) [23] or a
7Feedforward Neural Network Language Model (NNLM) [3] to an M T task is by rescoring the n-
best listsofa strongMTbaseline[22],whichreliablyimpro vestranslationquality.
More recently, researchershave begunto look into ways of in cluding informationabout the source
language into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLM
with a topic model of the input sentence, which improvesresc oring performance. Devlin et al. [8]
followed a similar approach, but they incorporatedtheir NN LM into the decoder of an MT system
and used the decoder’salignmentinformationto providethe NNLM with the most useful wordsin
the input sentence. Their approach was highly successful an d it achieved large improvementsover
theirbaseline.
Our work is closely related to Kalchbrennerand Blunsom [18] , who were the ﬁrst to map the input
sentence into a vector and then back to a sentence, although t hey map sentences to vectors using
convolutionalneuralnetworks,whichlosethe orderingoft he words. Similarlyto thiswork,Choet
al. [5] usedan LSTM-likeRNN architecturetomap sentencesi ntovectorsandback,althoughtheir
primaryfocuswasonintegratingtheirneuralnetworkintoa nSMTsystem. Bahdanauetal.[2]also
attempted direct translations with a neural network that us ed an attention mechanism to overcome
the poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging
results. Likewise, Pouget-Abadie et al. [26] attempted to a ddress the memory problem of Cho et
al. [5] by translatingpieces of the source sentence in way th at producessmoothtranslations, which
is similar to a phrase-basedapproach. We suspect that theyc ould achievesimilar improvementsby
simplytrainingtheirnetworksonreversedsourcesentence s.
End-to-endtrainingis also thefocusof Hermannet al. [12], whose modelrepresentsthe inputsand
outputsbyfeedforwardnetworks,andmapthemtosimilarpoi ntsinspace. However,theirapproach
cannotgeneratetranslationsdirectly: togetatranslatio n,theyneedtodoalookupforclosestvector
inthe pre-computeddatabaseofsentences,orto rescorea se ntence.
5 Conclusion
In this work, we showed that a large deep LSTM, that has a limit ed vocabulary and that makes
almostnoassumptionaboutproblemstructurecanoutperfor mastandardSMT-basedsystemwhose
vocabularyisunlimitedonalarge-scaleMTtask. Thesucces sofoursimpleLSTM-basedapproach
on MT suggests that it should do well on many other sequence le arning problems, provided they
haveenoughtrainingdata.
We were surprised by the extent of the improvementobtained b y reversing the words in the source
sentences. We concludethat it is importanttoﬁnd a probleme ncodingthat hasthe greatestnumber
of short term dependencies, as they make the learning proble m much simpler. In particular, while
we were unable to train a standard RNN on the non-reversedtra nslation problem (shown in ﬁg. 1),
we believe that a standard RNN should be easily trainable whe n the source sentences are reversed
(althoughwe didnotverifyit experimentally).
We were also surprised by the ability of the LSTM to correctly translate very long sentences. We
were initially convinced that the LSTM would fail on long sen tences due to its limited memory,
and other researchers reported poor performance on long sen tences with a model similar to ours
[5, 2, 26]. And yet, LSTMs trained on the reversed dataset had little difﬁculty translating long
sentences.
Most importantly, we demonstratedthat a simple, straightf orwardand a relatively unoptimizedap-
proach can outperform an SMT system, so further work will lik ely lead to even greater translation
accuracies. Theseresultssuggestthatourapproachwillli kelydowellonotherchallengingsequence
tosequenceproblems.
6 Acknowledgments
We thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hi nton, Nal Kalchbrenner, Thang Luong, Wolf-
gang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Woj ciech Zaremba, and the Google Brain team
for useful comments and discussions.
8References
[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint language and translation modeling with recurrent
neural networks. In EMNLP,2013.
[2] D.Bahdanau,K.Cho,andY.Bengio. Neuralmachinetransl ationbyjointlylearningtoalignandtranslate.
arXiv preprint arXiv:1409.0473 , 2014.
[3] Y.Bengio, R.Ducharme, P.Vincent,andC.Jauvin. Aneura l probabilistic language model. In Journal of
Machine Learning Research , pages 1137–1155, 2003.
[4] Y.Bengio,P.Simard,andP.Frasconi. Learninglong-ter mdependencies withgradientdescentisdifﬁcult.
IEEETransactions on Neural Networks ,5(2):157–166, 1994.
[5] K.Cho,B.Merrienboer,C.Gulcehre,F.Bougares,H.Schw enk,andY.Bengio. Learningphraserepresen-
tationsusingRNNencoder-decoderforstatisticalmachine translation. In ArxivpreprintarXiv:1406.1078 ,
2014.
[6] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column d eep neural networks for image classiﬁcation.
InCVPR,2012.
[7] G.E.Dahl,D.Yu,L.Deng, and A.Acero. Context-dependen t pre-traineddeepneural networks forlarge
vocabularyspeechrecognition. IEEETransactions onAudio,Speech,andLanguage Processin g-Special
Issue on DeepLearning for Speech andLanguage Processing , 2012.
[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and robust neural network
joint models for statisticalmachine translation. In ACL,2014.
[9] NadirDurrani, BarryHaddow, PhilippKoehn, andKenneth Heaﬁeld. Edinburgh’sphrase-based machine
translationsystems for wmt-14. In WMT, 2014.
[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850 ,
2013.
[11] A.Graves,S.Fern´ andez, F.Gomez,and J.Schmidhuber. Connectionist temporal classiﬁcation: labelling
unsegmented sequence data withrecurrent neural networks. InICML,2006.
[12] K. M. Hermann and P. Blunsom. Multilingual distributed representations without word alignment. In
ICLR,2014.
[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. Sainath, and B. Kingsbury. Deep neural networks for acous tic modeling in speech recognition. IEEE
Signal ProcessingMagazine , 2012.
[14] S. Hochreiter. Untersuchungen zu dynamischen neurona len netzen. Master's thesis, Institut fur Infor-
matik,Technische Universitat, Munchen , 1991.
[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhub er. Gradient ﬂow in recurrent nets: the difﬁculty
of learninglong-term dependencies, 2001.
[16] S.Hochreiter and J.Schmidhuber. Longshort-term memo ry.Neural Computation , 1997.
[17] S.Hochreiter and J.Schmidhuber. LSTMcansolve hardlo ng timelagproblems. 1997.
[18] N.Kalchbrenner andP.Blunsom. Recurrent continuous t ranslationmodels. In EMNLP,2013.
[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural
networks. In NIPS,2012.
[20] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. C orrado, J. Dean, and A.Y. Ng. Building
high-level features usinglarge scale unsupervised learni ng. InICML,2012.
[21] Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner. Gradient -basedlearningappliedtodocumentrecognition.
Proceedings of the IEEE ,1998.
[22] T. Mikolov. Statistical Language Models based on Neural Networks . PhD thesis, Brno University of
Technology, 2012.
[23] T. Mikolov, M. Karaﬁ´ at, L. Burget, J. Cernock` y, and S. Khudanpur. Recurrent neural network based
language model. In INTERSPEECH ,pages 1045–1048, 2010.
[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a met hod for automatic evaluation of machine
translation. In ACL,2002.
[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty o f training recurrent neural networks. arXiv
preprint arXiv:1211.5063 , 2012.
[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. C ho, and Y. Bengio. Overcoming the
curse of sentence length for neural machine translation usi ng automatic segmentation. arXiv preprint
arXiv:1409.1257 , 2014.
[27] A. Razborov. On small depth threshold circuits. In Proc. 3rd Scandinavian Workshop on Algorithm
Theory, 1992.
[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.
Nature, 323(6088):533–536, 1986.
[29] H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/ ˜schwenk/cslm_
joint_paper/ ,2014. [Online; accessed 03-September-2014].
[30] M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural net works for language modeling. In INTER-
SPEECH,2010.
[31] P.Werbos. Backpropagation through time: what itdoes a nd how todoit. Proceedings of IEEE ,1990.
9